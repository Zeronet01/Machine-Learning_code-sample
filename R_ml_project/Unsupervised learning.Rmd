---
title: "R machine learning code sample"
author: "LINGYI TAN"
date: "Mar 19, 2019"
output: word_document
---

#############################################################
Unsupervised machine learning techniques
#############################################################

#############################################################
Implement of Clustering
#############################################################

###
Global Human Freedom Index. The Cato Institute compiles a Human Freedom Index that measures personal, civil, and economic freedom in countries across the world. The index is compiled over a large number of individual freedom measures, each rated on a scale from 0 to 10, with 10 representing the most freedom.

In this problem, we’ll explore the use of principal components analysis (PCA) and clustering on the Cato 2017 Human Freedom Index for the subset of European countries included in the study

###1. Read in the dataset and conduct PCA to reduce dimension
```{r}
setwd("/Users/lingyitan/Desktop/hw6")
cato<-read.csv("europe_cato17.csv")
library("ggplot2")
summary(cato)#No need to clean
dim(cato)
#Remove non-numeric columns
cato_nu<-cato[,3:80]
#Remove constant columns
cato_nu1<-cato_nu[,sapply(cato_nu, function(v) var(v, na.rm=TRUE)!=0)]
#PCA
p1<-prcomp(cato_nu1,scale=TRUE)
summary(p1)
#Screenplot
screeplot(p1, type="lines")
#According to screenplot, 5 principal components should be selected to summarize the data since the slope between 5 and 6 is less steep then previous slopes.

#Create a new data frame with the projected features
df_new<-p1$x[,1:5]
row.names(df_new)<-cato[,1]
```

###2.Run k-means clustering with multiple initializations
```{R}
setwd("/Users/lingyitan/Desktop/hw6")
library("dendextend")
set.seed(1221)
k1 <- kmeans(scale(df_new), 4, nstart=100) # setting the number of random starting locations to 100
k1$centers
#Since principal components are uncorrelated with each others, perform clustering with pc will eliminate the strongly correlated variables in data set and thus the clustering result will not be influenced by the few correlated variables
europe<-read.csv("europeCoords.csv")
europe$region1<-toupper(substr(europe$region,1,3))
cluster<-as.data.frame(k1$cluster)
cluster$region1<-cato$ISO_Code
df1<-merge(cluster,europe,by="region1")
df1$cluster<-df1[,2]
europeCoords<-df1[,-2]

library(ggplot2)
ggplot() +
geom_polygon(data = europeCoords,
aes(x = long, y = lat, group = region, fill = cluster),color = "black", size = 0.1)
```
###3.Apply hierarchical clustering to the projected data. Compare the dendrograms obtained by single linkage and complete linkage.
```{R}
hc1 <- hclust(dist(scale(df_new)), method="single")
#unbalanced cluster
plot(hc1)
cluster<-as.data.frame(cutree(hc1, 4))
cluster$region1<-row.names(cluster)
df<-merge(cluster,europe,by="region1")
df$cluster<-df[,2]
europeCoords<-df[,-2]
ggplot() +
geom_polygon(data = europeCoords,
aes(x = long, y = lat, group = region, fill = cluster),color = "black", size = 0.1)
hc2 <- hclust(dist(scale(df_new)), method="complete")
plot(hc2)
#Cluster in single linkage are extremely unbalanced and spread out with only one county respectively in the last three clusters and all of the rest 36 countries classified into the first cluster. With clusters fused together, single linkage has a lower dissimilarity score (height).Cluster in complete linkage is much more balanced, with two clusters have over 10 counties and a greater height. 

cluster<-as.data.frame(cutree(hc2, 4))
cluster$region1<-row.names(cluster)
df<-merge(cluster,europe,by="region1")
df$cluster<-df[,2]
europeCoords<-df[,-2]
ggplot() +
geom_polygon(data = europeCoords,
aes(x = long, y = lat, group = region, fill = cluster),color = "black", size = 0.1)
```
Cluster memberships obtain from these two approaches are significantly different from that from k-means. Clusters in k-means are much more balanced than that in clustering. Most countries are classied into one or cluster and fewer countries are classified into the following clusters, and this situation is extremely pronounced at the single linkage in which the last three clusters have only one country repectively.




#############################################################
Implement of latent Dirichlet allocation (LDA) topic modeling
#############################################################

###
How the Russians Tweeted About the US Election.
On February 14, 2018, NBC released a dataset consisting of 200,000 Russian tweets linked to “malicious activity” intended to sway U.S. voters for the past presidential election. Here we will apply latent Dirichlet allocation (LDA) topic modeling to these tweets to identify common themes

###

###1. Load in the tweets dataset
```{R}
tweet<-read.csv("https://www.dropbox.com/s/wbrnfj3tt86ra24/tweets.csv?raw=1")
library(tm)
library(topicmodels)
text<-tweet$text
#Clean up the text
#Remove retweet entities
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", text)
#Rrmove At people
clean_tweet = gsub("@\\w+", "", clean_tweet)
# remove punctuation symbols
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
# remove numbers
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
# remove links
clean_tweet = gsub("http\\w+", "", clean_tweet)
# remove hashtags and other signs
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
clean_tweet = gsub("&amp", "", clean_tweet)
tweet1<-tweet
tweet1$text<-clean_tweet

```
###2. Pool together documents by author.
```{R}
head(tweet1)
tweets_by_author<-aggregate(text ~ user_id, data = tweet1, paste, collapse = " ")
dim(tweets_by_author)
#There are 393 documents in total
```
###3. Create a document term matrix for the tweets, run LDA on the reduced document term matrix with 3 topics
```{r}
library(tm)
library(topicmodels)
docs <- Corpus(VectorSource(tweet1$text))
docs <- Corpus(VectorSource(tweet1$text))
docs <- tm_map(docs, removePunctuation)
#Transform to lower case
docs <- tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs, removeNumbers)
save.image("~/docs.RData")
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs,stemDocument)
dtm <- DocumentTermMatrix(docs,control=list(bounds = list(global = c(10, Inf))))
inspect(dtm)
dtmmat <-as.matrix(dtm)

dammat1 <- dtmmat[rowSums(dtmmat)!=0,]
ave_tf <- colMeans(dammat1/rowSums(dammat1))
tfidf <- ave_tf * log(nrow(dammat1)/colSums(dammat1 > 0))
cutoff <- quantile(tfidf, 0.05)
dtm.common <- dammat1[,tfidf >= cutoff]
k=3
mod1 <- LDA(dtm.common, k, control=list(seed = 1221))
terms(mod1,10)

```
###4. Remove the words that are too common and again run LDA to regain 3 topics
```{R}
quantile(tfidf, 0.5)
quantile(tfidf, 0.90)
quantile(tfidf, 0.99)
length(tfidf)+1-rank(tfidf)[c("hillari","obama","trump")]
#“hillari”, “obama”, and “trump” rank in the 3rd, 4th and 1st this word popularity chart
cutoff1 <- quantile(tfidf, 0.99)
#Remove the words that are too common
dtm.reduced <- dammat1[,cutoff1 >= tfidf&tfidf >= cutoff]
dtm.reduced <- dtm.reduced[rowSums(dtm.reduced)!=0,]
ave_tf1 <- colMeans(dtm.reduced/rowSums(dtm.reduced))
tfidf1 <- ave_tf1 * log(nrow(dtm.reduced)/colSums(dtm.reduced > 0))
k=3
mod2 <- LDA(dtm.reduced, k, control=list(seed = 1221))
terms(mod2,10)
terms(mod1,10)
#Words in mod 1 are more about names and frequently-used verb such as get, make, want or say. We can't get lots of useful information from mod1 and we can't conclude what each topic is about. While words in mod2 show us lots of useful information and we can roughly know what each topic is about. The first topic may be about policy and law, the second is about terrorism and the third is about political party.
```
